{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mK4TJOFYv0h"
   },
   "source": [
    "## Assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_txt_lst = []\n",
    "file_label_lst = []\n",
    "for file in os.listdir(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\"):\n",
    "  with open(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file), 'r+',encoding=\"utf8\",errors=\"ignore\") as f:\n",
    "    doc_data = f.read()\n",
    "    original_txt_lst.append(doc_data)\n",
    "    label = re.split(\"_\",file)\n",
    "    file_label_lst.append(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjIiZ8JqOCnu",
    "outputId": "35a95eba-eb39-4dfd-9f8d-dbd638f16ba7"
   },
   "outputs": [],
   "source": [
    "# Preprocessing email\n",
    "def collect_email_replace():\n",
    "    total_email_dict = {}\n",
    "    for file in os.listdir(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\"):\n",
    "      try:\n",
    "          with open(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file), 'r+',encoding=\"utf8\",errors=\"ignore\") as f:\n",
    "            doc_data = f.read()\n",
    "            email_lst = re.findall(r\"[a-zA-Z0-9\\.\\-+_]+@[a-zA-Z0-9\\.\\-+_]+\", doc_data)\n",
    "            total_email_dict[file] = email_lst\n",
    "          if email_lst:\n",
    "            for email in email_lst:\n",
    "                doc_data = doc_data.replace(email, \" \")  \n",
    "                with open(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file), 'r+',encoding=\"utf8\",errors=\"ignore\") as f:\n",
    "                  f.write(doc_data)\n",
    "      except Exception as err:\n",
    "        print(err)\n",
    "    print(\"The below documents doesn't have email ids\")\n",
    "    for key in range(len(total_email_dict.keys())):\n",
    "      if not list(total_email_dict.items())[key][1]:\n",
    "        print(list(total_email_dict.items())[key][0])\n",
    "    return total_email_dict\n",
    "\n",
    "def process_email_words(total_email_dict):\n",
    "    email_words = []\n",
    "    for key in range(len(total_email_dict.keys())):\n",
    "      try:\n",
    "        each_doc_email_lst = list(total_email_dict.items())[key][1]\n",
    "        if each_doc_email_lst:\n",
    "          emailaddr_words = []\n",
    "          for email in each_doc_email_lst:\n",
    "            emailaddr = re.split(\"@\", email)[1:]\n",
    "            word_lst = re.split(\"\\\\.\", emailaddr[0])\n",
    "            count = 0\n",
    "            while count < len(word_lst):\n",
    "              if  len(word_lst[count]) <= 2 or word_lst[count] == 'com':\n",
    "                if count == 0:\n",
    "                    if count == len(word_lst[count]):\n",
    "                        word_lst = []\n",
    "                        break\n",
    "                    else:\n",
    "                        word_lst = word_lst[count+1:]    \n",
    "                else:\n",
    "                  word_lst1 = word_lst[:count]\n",
    "                  if count+1 < len(word_lst):\n",
    "                      word_lst2 = word_lst[count+1:]\n",
    "                      word_lst = word_lst1+word_lst2 \n",
    "                  else:\n",
    "                      word_lst = word_lst1\n",
    "                  count-=1\n",
    "              else:\n",
    "                count+=1\n",
    "            if word_lst:\n",
    "                emailaddr_words.extend(word_lst)\n",
    "          email_words.append(' '.join(item for item in emailaddr_words))\n",
    "        else:\n",
    "          email_words.append(' '.join(item for item in each_doc_email_lst))\n",
    "      except Exception as err:\n",
    "        print(email)\n",
    "        print(err)\n",
    "    return email_words\n",
    "    \n",
    "def preprocessed_emails():\n",
    "    email_dict = collect_email_replace()\n",
    "    email_wordslst = process_email_words(email_dict)\n",
    "    return email_wordslst  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing subjects\n",
    "def collect_subject_fro_write_tag_brkt_data():\n",
    "    all_sub_dict = {}\n",
    "    fro_write_dict = {}\n",
    "    tags_dict = {}\n",
    "    brkt_data_dict = {}\n",
    "    for file in os.listdir(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\"):\n",
    "      try:\n",
    "          with open(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file), 'r+',encoding=\"utf8\",errors=\"ignore\") as f:\n",
    "            doc_data1 = f.readlines()      \n",
    "            fro_write_lst = []\n",
    "            tags_lst = []\n",
    "            brkt_data_lst = []\n",
    "            for line in doc_data1:\n",
    "                if re.search(\"Subject:\", line):\n",
    "                    all_sub_dict[file] = line\n",
    "                fro = re.findall(r\"\\bFrom:\", line)\n",
    "                write = re.findall(r\"\\bWrite to:\", line)\n",
    "                if len(fro) >= 1 or len(write) >= 1:\n",
    "                    fro_write_lst.append(line)\n",
    "                tags = re.findall(r'\\<.*?\\>', line)\n",
    "                tags_lst.extend(tags)\n",
    "                brkt_data = re.findall(r'\\(.*?\\)', line)\n",
    "                brkt_data_lst.extend(brkt_data)\n",
    "            fro_write_dict[file] = fro_write_lst\n",
    "            tags_dict[file] = tags_lst\n",
    "            brkt_data_dict[file] = brkt_data_lst\n",
    "      except Exception as err:\n",
    "        print(err)\n",
    "    return all_sub_dict,fro_write_dict,tags_dict,brkt_data_dict\n",
    "\n",
    "def replace_subject_fro_write_tag_brkt_data(all_sub_dict,fro_write_dict,tags_dict,brkt_data_dict):\n",
    "    for i, file in enumerate(os.listdir(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\")):\n",
    "      try:\n",
    "          with open(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file), 'r+',encoding=\"utf8\",errors=\"ignore\") as f:\n",
    "            doc_data = f.read()\n",
    "            subject_lst = list(all_sub_dict.values())\n",
    "            if subject_lst[i]:\n",
    "                doc_data = doc_data.replace(subject_lst[i], \" \")  \n",
    "            fro_wri_lst = list(fro_write_dict.values())[i]\n",
    "            if fro_wri_lst:\n",
    "                for item in fro_wri_lst:\n",
    "                    doc_data = doc_data.replace(item, \" \")\n",
    "            tag_list = list(tags_dict.values())[i]\n",
    "            if tag_list:\n",
    "                for item in tag_list:\n",
    "                    doc_data = doc_data.replace(item, \" \")   \n",
    "            brk_dat_lst = list(brkt_data_dict.values())[i]\n",
    "            if brk_dat_lst:\n",
    "                for item in brk_dat_lst:\n",
    "                    doc_data = doc_data.replace(item, \" \")\n",
    "            with open(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file), 'r+',encoding=\"utf8\",errors=\"ignore\") as f:\n",
    "                f.write(doc_data)\n",
    "      except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "def process_subject_words(all_sub_dict):\n",
    "    all_sub_words = []\n",
    "    for key in range(len(all_sub_dict.keys())):\n",
    "        sub = list(all_sub_dict.items())[key][1]\n",
    "        if sub:\n",
    "            edit_sub = re.split(\"Subject:\", sub)\n",
    "            edit_sub = re.split(r\"\\s\", edit_sub[1])\n",
    "            sub_word_lst = []\n",
    "            for i, word in enumerate(edit_sub):\n",
    "                if word:\n",
    "                    edit_wrd_lst = re.split(r\"\\.\", word)\n",
    "                    if len(edit_wrd_lst)>1:\n",
    "                        for item in edit_wrd_lst:\n",
    "                            new_word = ''.join(filter(str.isalnum, item))\n",
    "                            sub_word_lst.append(new_word)\n",
    "                    else:\n",
    "                        new_word = ''.join(filter(str.isalnum, edit_wrd_lst[0]))\n",
    "                        sub_word_lst.append(new_word)\n",
    "            all_sub_words.append(' '.join(item for item in sub_word_lst))\n",
    "        else:\n",
    "            all_sub_words.append(' '.join(item for item in sub))\n",
    "    return all_sub_words\n",
    "\n",
    "def preprocessed_subject_fro_write_tag_brkt_data():\n",
    "    sub_dict,fro_write_dict,tag_dict,brkt_dict = collect_subject_fro_write_tag_brkt_data()\n",
    "    subject_wordlst = process_subject_words(sub_dict)\n",
    "    replace_subject_fro_write_tag_brkt_data(sub_dict,fro_write_dict,tag_dict,brkt_dict)\n",
    "    return subject_wordlst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PRASAD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\PRASAD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\PRASAD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing text \n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Refered from https://stackoverflow.com/a/47091490/4084039\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def word_processing():\n",
    "    for file in os.listdir(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\"):\n",
    "      try:\n",
    "        with open(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file), 'r+',encoding=\"utf8\",errors=\"ignore\") as f:\n",
    "            doc_data1 = f.readlines()  \n",
    "            line_lst = []\n",
    "            for line in doc_data1:\n",
    "                if line != \"\\n\":\n",
    "                    line = decontracted(line)\n",
    "                    wordslst = re.split(\"\\s\",line)\n",
    "                    if wordslst:\n",
    "                        edited_word_lst = []\n",
    "                        for word in wordslst:\n",
    "                            if (word != \"\"):\n",
    "                                if re.search(\":\", word):\n",
    "                                    word = \"\"\n",
    "                                else:\n",
    "                                    word = re.sub('[^A-Za-z]+', ' ', word)\n",
    "                                if len(word) > 3 and len(word)<16:\n",
    "                                    edited_word_lst.append(word)\n",
    "                                else:\n",
    "                                    edited_word_lst.append(\"\")\n",
    "                        line = \" \".join(edited_word_lst)\n",
    "                    line_lst.append(line)\n",
    "        with open(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", \"temp\"), 'w+',encoding=\"utf8\",errors=\"ignore\") as f1:\n",
    "            for line in line_lst:\n",
    "                f1.write(line)\n",
    "                f1.write(\"\\n\")\n",
    "        os.remove(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file))\n",
    "        os.rename(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", \"temp\"), os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file))\n",
    "      except Exception as err:\n",
    "        print(err) \n",
    "\n",
    "def chunking():\n",
    "    # The below code refered from https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list/31837224#31837224\n",
    "    for file in os.listdir(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\"):\n",
    "      try:\n",
    "          with open(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file), 'r+',encoding=\"utf8\",errors=\"ignore\") as f:\n",
    "            doc_data = f.read()      \n",
    "            parse_tree = nltk.ne_chunk(nltk.tag.pos_tag(doc_data.split()), binary=True)  # POS tagging before chunking!\n",
    "            for t in parse_tree.subtrees():\n",
    "                if t.label() == 'PERSON':\n",
    "                    if len(t)>=2:\n",
    "                        for name in t:\n",
    "                            doc_data = doc_data.replace(name[0], \" \")\n",
    "                    else:\n",
    "                        doc_data = doc_data.replace(t[0][0], \" \")\n",
    "                if t.label() == 'GPE':\n",
    "                    if len(t)>=2:\n",
    "                        for item in t:\n",
    "                            old_word = ' '.join(item)\n",
    "                            new_word = '_'.join(item)\n",
    "                        doc_data = doc_data.replace(old_word, new_word) \n",
    "            with open(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file), 'r+',encoding=\"utf8\",errors=\"ignore\") as f:\n",
    "                f.write(doc_data.lower())\n",
    "      except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "def preprocessed_text():\n",
    "    for i in range(3):\n",
    "        word_processing()\n",
    "    chunking()\n",
    "    preprocessed_txt_lst = []\n",
    "    for file in os.listdir(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\"):\n",
    "      with open(os.path.join(\"D:\\Applied_AI\\Assignments\\Assignment_21\\documents\", file), 'r+',encoding=\"utf8\",errors=\"ignore\") as f:\n",
    "        doc_data = f.read()\n",
    "        preprocessed_txt_lst.append(doc_data)\n",
    "    return preprocessed_txt_lst\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The below documents doesn't have email ids\n",
      "alt.atheism_53121.txt\n",
      "alt.atheism_53806.txt\n",
      "comp.os.ms-windows.misc_9671.txt\n",
      "comp.sys.ibm.pc.hardware_61034.txt\n",
      "comp.sys.mac.hardware_51507.txt\n",
      "comp.sys.mac.hardware_51904.txt\n",
      "comp.sys.mac.hardware_52152.txt\n",
      "rec.sport.baseball_104352.txt\n",
      "rec.sport.baseball_104418.txt\n",
      "rec.sport.baseball_104471.txt\n",
      "rec.sport.hockey_53642.txt\n",
      "rec.sport.hockey_53671.txt\n"
     ]
    }
   ],
   "source": [
    "def preprocess():\n",
    "    preprocessed_email = preprocessed_emails()\n",
    "    preprocessed_subject = preprocessed_subject_fro_write_tag_brkt_data()\n",
    "    preprocess_text = preprocessed_text()\n",
    "    return preprocessed_email,preprocessed_subject,preprocess_text\n",
    "pre_email,pre_subj,pre_text = preprocess()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources\n",
      "december\n",
      "atheist resources\n",
      "addresses atheist organizations\n",
      "freedom from religion foundation\n",
      "darwin fish bumper stickers assorted other atheist paraphernalia\n",
      "available from freedom from religion foundation\n",
      "evolution designs\n",
      "evolution designs sell darwin fish fish symbol like ones\n",
      "christians stick their cars with feet word darwin written\n",
      "inside deluxe moulded plastic fish postpaid\n",
      "people francisco area darwin fish from lynn gold\n",
      "mailing people lynn directly\n",
      "price fish\n",
      "american atheist press\n",
      "publish various atheist books critiques bible lists\n",
      "biblical contradictions such book\n",
      "bible handbook ball foote american atheist press\n",
      "isbn edition bible contradictions\n",
      "absurdities atrocities immoralities contains ball bible\n",
      "contradicts itself based king james version bible\n",
      "cameron road austin\n",
      "prometheus books\n",
      "sell books including haught holy horrors\n",
      "alternate address\n",
      "prometheus books glenn drive buffalo\n",
      "humanism\n",
      "organization promoting black secular humanism uncovering history\n",
      "black freethought they publish quarterly newsletter examiner\n",
      "buffalo\n",
      "united kingdom\n",
      "rationalist press association national secular society\n",
      "islington high street holloway road\n",
      "london london\n",
      "british humanist association south place ethical society\n",
      "lamb conduit passage conway hall\n",
      "london lion square\n",
      "london\n",
      "national secular society publish freethinker monthly magazine\n",
      "founded\n",
      "germany\n",
      "ibka\n",
      "internationaler bund atheisten\n",
      "postfach berlin germany\n",
      "ibka publish\n",
      "materialien informationen zeit politisches\n",
      "journal atheisten hrsg ibka\n",
      "vertrieb postfach berlin germany\n",
      "atheist books write\n",
      "ibdk internationaler ucherdienst\n",
      "postfach hannover germany\n",
      "books fiction\n",
      "thomas disch\n",
      "santa claus compromise\n",
      "short story ultimate proof that santa exists characters\n",
      "events fictitious similarity living dead gods well\n",
      "walter miller\n",
      "canticle leibowitz\n",
      "this post atomic doomsday novel monks spent their lives\n",
      "copying blueprints from saint leibowitz filling sheets paper with\n",
      "leaving white lines letters\n",
      "edgar pangborn\n",
      "davy\n",
      "post atomic doomsday novel clerical states church example\n",
      "forbids that anyone produce describe substance containing\n",
      "atoms\n",
      "philip dick\n",
      "philip dick dick wrote many philosophical short\n",
      "stories novels stories bizarre times very approachable\n",
      "wrote mainly wrote about people truth religion rather than\n",
      "technology although often believed that some sort\n",
      "remained sceptical amongst novels following some\n",
      "galactic healer\n",
      "fallible alien deity summons group earth craftsmen women\n",
      "remote planet raise giant cathedral from beneath oceans when\n",
      "deity begins demand faith from earthers healer fernwright\n",
      "unable comply polished ironic amusing novel\n",
      "maze death\n",
      "noteworthy description religion\n",
      "valis\n",
      "schizophrenic hero searches hidden mysteries gnostic\n",
      "christianity after reality fired into brain pink laser beam\n",
      "unknown possibly divine origin accompanied dogmatic\n",
      "dismissively atheist friend assorted other characters\n",
      "divine invasion\n",
      "invades earth making young woman pregnant returns from\n",
      "another star system unfortunately terminally must\n",
      "assisted dead whose brain wired hour easy listening music\n",
      "margaret atwood\n",
      "handmaid tale\n",
      "story based premise that congress mysteriously\n",
      "assassinated fundamentalists quickly take charge nation\n",
      "right again book diary woman life tries live\n",
      "under christian theocracy women right property revoked\n",
      "their bank accounts closed sinful luxuries outlawed\n",
      "radio only used readings from bible crimes punished\n",
      "doctors performed legal abortions world\n",
      "hunted down hanged atwood writing style difficult used\n",
      "first tale grows more more chilling goes\n",
      "various authors\n",
      "bible\n",
      "this somewhat dull rambling work often been criticized however\n",
      "probably worth reading only that will know what fuss\n",
      "about exists many different versions make sure\n",
      "true version\n",
      "books fiction\n",
      "peter rosa\n",
      "vicars christ bantam press\n",
      "although rosa seems christian even catholic this very\n",
      "enlighting history papal immoralities adulteries fallacies\n",
      "german gottes erste diener dunkle seite papsttums\n",
      "droemer knaur\n",
      "michael martin\n",
      "philosophical justification temple university press\n",
      "philadelphia\n",
      "detailed scholarly justification atheism contains outstanding\n",
      "appendix defining terminology usage this tendentious\n",
      "area argues both negative atheism belief\n",
      "existence also positive atheism belief\n",
      "existence includes great refutations most\n",
      "challenging arguments particular attention paid refuting\n",
      "contempory theists such platinga swinburne\n",
      "pages isbn\n",
      "case against christianity temple university press\n",
      "comprehensive critique christianity which considers\n",
      "best contemporary defences christianity\n",
      "demonstrates that they unsupportable incoherent\n",
      "pages isbn\n",
      "james turner\n",
      "without without creed johns hopkins university press baltimore\n",
      "subtitled origins unbelief america examines which\n",
      "unbelief became mainstream alternative\n",
      "world view focusses period while considering france\n",
      "britain emphasis american particularly england\n",
      "developments neither religious history secularization atheism\n",
      "without without creed rather intellectual history fate\n",
      "single idea belief that exists\n",
      "pages isbn\n",
      "george seldes\n",
      "great thoughts ballantine books york\n",
      "dictionary quotations different kind concentrating statements\n",
      "writings which explicitly implicitly present person philosophy\n",
      "world view includes obscure opinions from many\n",
      "people some popular observations traces which various\n",
      "people expressed twisted idea over centuries quite number\n",
      "quotations derived from cardiff what great think religion\n",
      "noyes views religion\n",
      "pages isbn\n",
      "richard swinburne\n",
      "existence clarendon paperbacks oxford\n",
      "this book second volume trilogy that began with coherence\n",
      "theism concluded with faith reason this\n",
      "work swinburne attempts construct series inductive arguments\n",
      "existence arguments which somewhat tendentious rely\n",
      "upon imputation late century western christian values\n",
      "aesthetics which supposedly simple conceived were\n",
      "decisively rejected mackie miracle theism revised\n",
      "edition existence swinburne includes appendix which\n",
      "makes somewhat incoherent attempt rebut mackie\n",
      "mackie\n",
      "miracle theism oxford\n",
      "this volume contains comprehensive review principal\n",
      "arguments against existence ranges from classical\n",
      "philosophical positions descartes anselm berkeley hume through\n",
      "moral arguments newman kant sidgwick recent restatements\n",
      "classical theses plantinga swinburne also addresses those\n",
      "positions which push concept beyond realm rational\n",
      "such those kierkegaard kung philips well replacements\n",
      "such lelie axiarchism book delight read less\n",
      "formalistic better written than martin works refreshingly direct\n",
      "when compared with hand waving swinburne\n",
      "james haught\n",
      "holy illustrated history religious murder madness\n",
      "prometheus books\n",
      "looks religious persecution from ancient times present\n",
      "only christians\n",
      "library congress catalog card number\n",
      "norm allen\n",
      "african american anthology\n",
      "listing african americans humanism above\n",
      "gordon stein\n",
      "anthology atheism rationalism prometheus books\n",
      "anthology covering wide range subjects including devil evil\n",
      "morality history freethought comprehensive bibliography\n",
      "edmund cohen\n",
      "mind bible believer prometheus books\n",
      "study people become christian what effect\n",
      "them\n",
      "resources\n",
      "there small mail based archive server mantis which carries\n",
      "archives articles assorted other files\n",
      "more information send mail saying\n",
      "help\n",
      "send atheism index\n",
      "will mail back reply\n",
      "mathew\n",
      "reply\n",
      "mathew\n",
      "mathew\n",
      "devil evil\n",
      "morality history freethought comprehensive bibliography\n",
      "edmund cohen\n",
      "mind bible believer prometheus books\n",
      "study people become christian what effect\n",
      "them\n",
      "resources\n",
      "there small mail based archive server mantis which carries\n",
      "archives articles assorted other files\n",
      "more information send mail saying\n",
      "help\n",
      "send atheism index\n",
      "will mail back reply\n",
      "mathew\n",
      "reply\n",
      "mathew\n",
      "mathew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pre_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"text\": original_txt_lst,\n",
    "  \"class\": file_label_lst,\n",
    "  \"preprocessed_text\": pre_text,\n",
    "  \"preprocessed_subject\": pre_subj,\n",
    "  \"preprocessed_emails\": pre_email,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hB43OGEfYv1C",
    "outputId": "945bc8a4-1f99-4410-94c8-c776a405b5f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'class', 'preprocessed_text', 'preprocessed_subject',\n",
       "       'preprocessed_emails'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AM6A19xFYv1I",
    "outputId": "9de13fa8-6604-49a2-8013-6b22f0a256a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                    From: perry@dsinc.com (Jim Perry)\\nSubject: Re...\n",
       "class                                                         alt.atheism\n",
       "preprocessed_text       this response originally fell into bucket repo...\n",
       "preprocessed_subject         Re Is Morality Constant was Re Biblical Rape\n",
       "preprocessed_emails     dsinc darkside osrhe uoknor edu okcforum osrhe...\n",
       "Name: 400, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preprocess_file\"] = df[\"preprocessed_text\"] + df[\"preprocessed_subject\"] + df[\"preprocessed_emails\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>preprocessed_subject</th>\n",
       "      <th>preprocessed_emails</th>\n",
       "      <th>preprocess_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: A...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>resources\\ndecember\\natheist resources\\naddres...</td>\n",
       "      <td>Alt Atheism FAQ Atheist Resources</td>\n",
       "      <td>mantis netcom mantis</td>\n",
       "      <td>resources\\ndecember\\natheist resources\\naddres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class  \\\n",
       "0  From: mathew <mathew@mantis.co.uk>\\nSubject: A...  alt.atheism   \n",
       "\n",
       "                                   preprocessed_text  \\\n",
       "0  resources\\ndecember\\natheist resources\\naddres...   \n",
       "\n",
       "                preprocessed_subject   preprocessed_emails  \\\n",
       "0  Alt Atheism FAQ Atheist Resources  mantis netcom mantis   \n",
       "\n",
       "                                     preprocess_file  \n",
       "0  resources\\ndecember\\natheist resources\\naddres...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[\"preprocess_file\"])\n",
    "y = np.array(df[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14121,), (4707,), (14121,), (4707,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((14121, 1000), (4707, 1000))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_X_train = tokenizer.texts_to_sequences(X_train)\n",
    "prepad_sequence_X_train = pad_sequences(sequences_X_train, maxlen=1000)\n",
    "sequences_X_test = tokenizer.texts_to_sequences(X_test)\n",
    "prepad_sequence_X_test = pad_sequences(sequences_X_test, maxlen=1000)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "prepad_sequence_X_train.shape, prepad_sequence_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14121, 20), (4707, 20))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(y_train.reshape(-1, 1))\n",
    "y_train_onhot = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_onhot = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "y_train_onhot.shape, y_test_onhot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code is referenced from https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "# load the whole embedding into memory\n",
    "from numpy import asarray\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('D:\\Applied_AI\\Assignments\\Assignment_21/glove.6B.300d.txt', encoding=\"utf8\", errors=\"ignore\")\n",
    "doc_data = f.readlines()\n",
    "for line in doc_data:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "from numpy import zeros\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are other ways of doing this: https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/ you can try this way also\n",
    "%load_ext tensorboard\n",
    "# Clear any logs from previous runs\n",
    "#del -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class microf1_score(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,validation_data):\n",
    "      self.x_test = validation_data[0]\n",
    "      self.y_test= validation_data[1]\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.history={'val_f1score': []}\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred= self.model.predict(self.x_test)\n",
    "        #y_label_pred=np.argmax(y_pred,axis=1)\n",
    "        f1score = f1_score(self.y_test,y_pred,average='micro')\n",
    "        self.history['val_f1score'].append(f1score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "221/221 [==============================] - 403s 2s/step - loss: 2.2611 - accuracy: 0.2370 - val_loss: 1.5440 - val_accuracy: 0.4417\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.44168, saving model to .\\best_model-01-0.4417.h5\n",
      "Epoch 2/100\n",
      "221/221 [==============================] - 374s 2s/step - loss: 1.3115 - accuracy: 0.5350 - val_loss: 1.1446 - val_accuracy: 0.5949\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.44168 to 0.59486, saving model to .\\best_model-02-0.5949.h5\n",
      "Epoch 3/100\n",
      "221/221 [==============================] - 394s 2s/step - loss: 0.9557 - accuracy: 0.6632 - val_loss: 0.9689 - val_accuracy: 0.6745\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.59486 to 0.67453, saving model to .\\best_model-03-0.6745.h5\n",
      "Epoch 4/100\n",
      "221/221 [==============================] - 368s 2s/step - loss: 0.6941 - accuracy: 0.7539 - val_loss: 0.9312 - val_accuracy: 0.7121\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.67453 to 0.71213, saving model to .\\best_model-04-0.7121.h5\n",
      "Epoch 5/100\n",
      "221/221 [==============================] - 377s 2s/step - loss: 0.5084 - accuracy: 0.8206 - val_loss: 0.9236 - val_accuracy: 0.7317\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.71213 to 0.73168, saving model to .\\best_model-05-0.7317.h5\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26450314a20>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import Activation, Dense, Input, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "input_layer = Input(shape=(1000,))\n",
    "embed_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=1000, trainable=False)(input_layer)\n",
    "layer3_1 = Conv1D(32,5,activation='relu')(embed_layer)\n",
    "layer3_2 = Conv1D(32,5,activation='relu')(embed_layer)\n",
    "layer3_3 = Conv1D(32,5,activation='relu')(embed_layer)\n",
    "layer3_4 = Conv1D(32,5,activation='relu')(embed_layer)\n",
    "layer4 = tf.keras.layers.Concatenate()([layer3_1, layer3_2, layer3_3, layer3_4])\n",
    "maxpool1 = tf.keras.layers.MaxPooling1D(5)(layer4)\n",
    "layer6_1 = Conv1D(32,5,activation='relu')(maxpool1)\n",
    "layer6_2 = Conv1D(32,5,activation='relu')(maxpool1)\n",
    "layer6_3 = Conv1D(32,5,activation='relu')(maxpool1)\n",
    "layer6_4 = Conv1D(32,5,activation='relu')(maxpool1)\n",
    "layer7 = tf.keras.layers.Concatenate()([layer6_1, layer6_2, layer6_3, layer6_4])\n",
    "maxpool2 = tf.keras.layers.MaxPooling1D(5)(layer7)\n",
    "layer9_1 = Conv1D(32,5,activation='relu')(maxpool2)\n",
    "layer9_2 = Conv1D(32,5,activation='relu')(maxpool2)\n",
    "layer9_3 = Conv1D(32,5,activation='relu')(maxpool2)\n",
    "layer9_4 = Conv1D(32,5,activation='relu')(maxpool2)\n",
    "layer10 = tf.keras.layers.Concatenate()([layer9_1, layer9_2, layer9_3, layer9_4])\n",
    "maxpool3 = tf.keras.layers.MaxPooling1D(5)(layer10)\n",
    "layer12 = Conv1D(32,5,activation='relu')(maxpool3)\n",
    "flatten = Flatten()(layer12)\n",
    "dropout = tf.keras.layers.Dropout(0.1)(flatten)\n",
    "layer16 = Dense(128,activation='relu')(dropout)\n",
    "output = Dense(20,activation='softmax')(layer16)\n",
    "\n",
    "model1 = Model(inputs=input_layer,outputs=output)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model1.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "filepath=\"./best_model-{epoch:02d}-{val_accuracy:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.02, patience=1, verbose=1)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "model1.fit(prepad_sequence_X_train,y_train_onhot,epochs=100,validation_data=(prepad_sequence_X_test,y_test_onhot),batch_size=64,callbacks=[checkpoint, earlystop, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 20492), started 0:05:46 ago. (Use '!kill 20492' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3ef64a577b0014bf\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3ef64a577b0014bf\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = Tokenizer(num_words=None,char_level = True, oov_token='UNK')\n",
    "tokenizer1.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 1,\n",
       " ' ': 2,\n",
       " 'e': 3,\n",
       " 't': 4,\n",
       " 'a': 5,\n",
       " 'i': 6,\n",
       " 's': 7,\n",
       " 'r': 8,\n",
       " 'n': 9,\n",
       " 'o': 10,\n",
       " 'l': 11,\n",
       " 'h': 12,\n",
       " 'c': 13,\n",
       " 'd': 14,\n",
       " '\\n': 15,\n",
       " 'u': 16,\n",
       " 'm': 17,\n",
       " 'p': 18,\n",
       " 'g': 19,\n",
       " 'w': 20,\n",
       " 'y': 21,\n",
       " 'b': 22,\n",
       " 'f': 23,\n",
       " 'v': 24,\n",
       " 'k': 25,\n",
       " 'x': 26,\n",
       " 'j': 27,\n",
       " 'z': 28,\n",
       " 'q': 29,\n",
       " '1': 30,\n",
       " '0': 31,\n",
       " '2': 32,\n",
       " '-': 33,\n",
       " '3': 34,\n",
       " '4': 35,\n",
       " '6': 36,\n",
       " '5': 37,\n",
       " '8': 38,\n",
       " '9': 39,\n",
       " '7': 40,\n",
       " '_': 41,\n",
       " '+': 42}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((14121, 1000), (4707, 1000))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq = tokenizer1.texts_to_sequences(X_train)\n",
    "X_train_pad_seq = pad_sequences(X_train_seq, maxlen=1000)\n",
    "X_test_seq = tokenizer1.texts_to_sequences(X_test)\n",
    "X_test_pad_seq = pad_sequences(X_test_seq, maxlen=1000)\n",
    "vocab_size = len(tokenizer1.word_index) + 1\n",
    "print(vocab_size)\n",
    "X_train_pad_seq.shape, X_test_pad_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code is referenced from https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "# load the whole embedding into memory\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('D:\\Applied_AI\\Assignments\\Assignment_21\\glove.840B.300d-char.txt', encoding=\"utf8\", errors=\"ignore\")\n",
    "doc_data = f.readlines()\n",
    "for line in doc_data:\n",
    "    values = line.split()\n",
    "    character = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[character] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "from numpy import zeros\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, i in tokenizer1.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "221/221 [==============================] - 129s 580ms/step - loss: 2.9458 - accuracy: 0.0788 - val_loss: 2.9126 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.08328, saving model to .\\best_model-01-0.0833.h5\n",
      "Epoch 2/100\n",
      "221/221 [==============================] - 127s 577ms/step - loss: 2.9080 - accuracy: 0.0910 - val_loss: 2.8751 - val_accuracy: 0.0952\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.08328 to 0.09518, saving model to .\\best_model-02-0.0952.h5\n",
      "Epoch 00002: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x263c2e269b0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_c = Input(shape=(1000,))\n",
    "embed_layer_c = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=1000, trainable=False)(input_layer_c)\n",
    "layer3_c = Conv1D(32,5,activation='relu')(embed_layer_c)\n",
    "layer4_c_1 = Conv1D(32,5,activation='relu')(layer3_c)\n",
    "layer4_c_2 = Conv1D(32,5,activation='relu')(layer3_c)\n",
    "concatlayer = tf.keras.layers.Concatenate()([layer4_c_1, layer4_c_2])\n",
    "maxpool1_c = tf.keras.layers.MaxPooling1D(5)(concatlayer)\n",
    "layer6_c = Conv1D(32,5,activation='relu')(maxpool1_c)\n",
    "layer7_c = Conv1D(32,5,activation='relu')(layer6_c)\n",
    "maxpool2_c = tf.keras.layers.MaxPooling1D(5)(layer7_c)\n",
    "flatten_c = Flatten()(maxpool2_c)\n",
    "dropout_c = tf.keras.layers.Dropout(0.1)(flatten_c)\n",
    "layer11_c = Dense(128,activation='relu')(dropout_c)\n",
    "output_c = Dense(20,activation='softmax')(layer11_c)\n",
    "\n",
    "model2 = Model(inputs=input_layer_c,outputs=output_c)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model2.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "filepath=\"./best_model-{epoch:02d}-{val_accuracy:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.02, patience=1, verbose=1)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./Char_logs\")\n",
    "\n",
    "model2.fit(X_train_pad_seq,y_train_onhot,epochs=100,validation_data=(X_test_pad_seq,y_test_onhot),batch_size=64,callbacks=[checkpoint, earlystop, tensorboard_callback])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
